# -DA-project-consultas-SQL_JOAQUINMF
Repo para el ejercicio consultas_SQL

# Proyecto de Limpieza y Procesamiento de Datos â€” Base de Datos Sakila

Este proyecto consiste en la extracciÃ³n, limpieza preliminar (SQL) y procesamiento final (Python) de datos provenientes de la base de datos **Sakila**.  
El objetivo es construir tres dataframes mediante consultas SQL, seleccionar uno para aplicar limpieza exhaustiva y luego procesarlo en un notebook de Google Colab.

---

# ğŸ“ Estructura del Proyecto

/sql/
sakila-schema.sql
sakila-data.sql
extraction_dataframes.sql
cleaning_dataframe1.sql

/notebook/
limpieza_final.ipynb

/data/
df_clientes_limpio.csv
df_cliente_final.csv

---


---

# ğŸ—ƒï¸ Dataframes Generados desde SQL

Se generaron tres dataframes mediante joins entre tablas de la base de datos Sakila:

### **1. Actividad de clientes**  
Tablas: `customer`, `address`, `city`, `country`, `rental`, `payment`  
Objetivo: Describir el comportamiento de alquileres y pagos por cliente.

### **2. CatÃ¡logo de pelÃ­culas**  
Tablas: `film`, `film_category`, `category`, `language`, `inventory`  
Objetivo: Crear una vista del catÃ¡logo con categorÃ­as, idioma y disponibilidad.

### **3. Elenco y popularidad**  
Tablas: `film`, `actor`, `film_actor`  
Objetivo: Analizar actores por pelÃ­cula y frecuencia de apariciÃ³n.

El dataframe **seleccionado para limpieza completa** fue:

ğŸ‘‰ **Dataframe 1 â€” Actividad de clientes**

---

# ğŸ§© 1. Pasos SQL del Proyecto

## âœ” InstalaciÃ³n de la base de datos Sakila
1. Importar `sakila-schema.sql` en el servidor MySQL  
2. Importar `sakila-data.sql`  
3. Confirmar que las tablas fueron creadas correctamente

## âœ” ExtracciÃ³n de los tres dataframes  
Las consultas fueron incluidas en `extraction_dataframes.sql`.  
Se utilizaron **joins basados en claves primarias y forÃ¡neas** para garantizar integridad.

## âœ” Limpieza SQL del Dataframe seleccionado  
En el archivo `cleaning_dataframe1.sql` se realizÃ³:

- EstandarizaciÃ³n de texto con `LOWER()` y `TRIM()`
- EliminaciÃ³n de alquileres sin pago o sin devoluciÃ³n
- Filtrado de montos de pago positivos
- EliminaciÃ³n de registros con `payment_id` o `rental_id` nulos
- ValidaciÃ³n de consistencia temporal (`rental_date < return_date`)
- CreaciÃ³n de la columna derivada `rental_duration` con `DATEDIFF()`

## âœ” ExportaciÃ³n del DataFrame limpio  
El resultado final fue exportado como `df_clientes_limpio.csv` desde MySQL Workbench.

---

# ğŸ§¹ 2. Criterios de Limpieza Aplicados

La limpieza se realizÃ³ en dos etapas: **SQL** (preliminar) y **Python** (final).

## ğŸ—„ï¸ Limpieza en SQL
- ConversiÃ³n a minÃºsculas y eliminaciÃ³n de espacios en campos de texto  
- EliminaciÃ³n de registros con datos clave faltantes  
- ValidaciÃ³n de integridad referencial  
- Filtrado de pagos invÃ¡lidos (`amount > 0`)  
- Filtrado de alquileres completados (`return_date IS NOT NULL`)

## ğŸ Limpieza en Python
- ConversiÃ³n de columnas de fecha a formato datetime  
- NormalizaciÃ³n de cadenas: `lower()`, `strip()`  
- ConversiÃ³n de tipos numÃ©ricos (float, int)  
- DetecciÃ³n y eliminaciÃ³n de duplicados  
- RevisiÃ³n de valores faltantes y documentaciÃ³n de decisiones  
- DetecciÃ³n y eliminaciÃ³n de *outliers* simples por Z-score  
- CreaciÃ³n de columnas derivadas (Â«payment_monthÂ», Â«customer_full_nameÂ»)  
- VisualizaciÃ³n para revisar distribuciones  
- ExportaciÃ³n final del dataset en CSV y Parquet  

---

# ğŸ§ª 3. Instrucciones para Ejecutar el Notebook

1. Abrir **Google Colab**
2. Cargar el archivo `df_clientes_limpio.csv`  
   - O bien montar Google Drive
3. Ejecutar todas las celdas del notebook en orden:
   - **Carga del dataset**
   - **ConversiÃ³n de fechas**
   - **NormalizaciÃ³n y limpieza**
   - **DetecciÃ³n de outliers**
   - **Columnas derivadas**
   - **Visualizaciones**
4. Confirmar que la Ãºltima celda exporta los archivos:
   - `df_cliente_final.csv`
   - `df_cliente_final.parquet`

El notebook incluye comentarios explicando cada etapa.

---

# ğŸ§  4. DescripciÃ³n de Decisiones Tomadas

Durante el proyecto se tomaron decisiones basadas en criterios de calidad de datos y consistencia:

### âœ” ElecciÃ³n del dataframe para limpieza completa  
Se seleccionÃ³ el **Dataframe 1** debido a su complejidad y utilidad analÃ­tica (actividad y pagos de clientes).

### âœ” EstandarizaciÃ³n de cadenas  
Se decidiÃ³ normalizar todo en minÃºsculas para evitar problemas de duplicaciÃ³n y comparaciones inconsistentes.

### âœ” ExclusiÃ³n de registros incompletos  
Se excluyeron alquileres sin devoluciÃ³n y pagos sin monto vÃ¡lido, ya que estos no reflejan actividad real del cliente.

### âœ” CorrecciÃ³n de fechas  
Se impuso la regla `rental_date < return_date` para eliminar registros corruptos.

### âœ” EliminaciÃ³n de outliers en Python  
Se utilizÃ³ Z-score simple en Â«amountÂ» para detectar valores atÃ­picos y mejorar la distribuciÃ³n del dataset.

### âœ” GeneraciÃ³n de columnas derivadas  
Se aÃ±adieron atributos Ãºtiles como:
- `rental_duration` (SQL)
- `payment_month` (Python)
- `customer_full_name` (Python)

Estas columnas facilitan futuros anÃ¡lisis temporales y de comportamiento.

---

# ğŸ¯ ConclusiÃ³n

El proyecto permite:

- Comprender la estructura de Sakila  
- Practicar joins complejos  
- Aplicar limpieza en SQL y Python de forma combinada  
- Documentar un flujo reproducible  
- Obtener un dataset final apto para anÃ¡lisis posteriores  
